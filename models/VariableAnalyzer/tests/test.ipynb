{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray_beam as xb\n",
    "ds, _ = xb.open_zarr('gcs://era5_preprocess/1440x720/2006-12-31_2011-12-31.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131331.98 131333.22 131328.61 131321.23 131306.78 131286.48 131271.11\n",
      "  131260.66 131255.12 131261.27 131274.19 131280.03 131274.48 131256.66\n",
      "  131227.14 131201.   131200.69 131227.75 131268.64 131308.62 131334.75\n",
      "  131342.14 131339.67 131339.06 131343.67 131354.75 131371.05 131387.03]\n",
      " [131473.12 131473.73 131470.05 131462.67 131449.14 131430.08 131409.17\n",
      "  131390.72 131385.48 131396.56 131407.62 131411.62 131417.16 131414.1\n",
      "  131386.72 131351.67 131339.06 131350.75 131375.03 131412.25 131453.75\n",
      "  131477.12 131479.28 131477.12 131480.5  131489.11 131503.27 131519.56]\n",
      " [131615.48 131615.48 131612.11 131605.66 131596.12 131580.75 131556.77\n",
      "  131533.7  131526.62 131534.62 131542.61 131548.77 131561.69 131567.52\n",
      "  131545.08 131509.72 131490.66 131488.5  131490.03 131508.48 131554.61\n",
      "  131598.27 131612.72 131612.72 131618.25 131628.72 131642.55 131659.16]\n",
      " [131758.47 131758.16 131755.7  131751.1  131745.25 131732.03 131709.27\n",
      "  131688.06 131677.61 131678.53 131684.98 131694.52 131705.58 131716.03\n",
      "  131710.19 131681.6  131653.   131640.1  131628.72 131622.56 131649.62\n",
      "  131699.73 131733.25 131743.72 131754.17 131770.16 131787.06 131804.6 ]\n",
      " [131900.53 131902.06 131901.77 131898.69 131894.08 131881.77 131862.7\n",
      "  131846.11 131834.11 131829.19 131836.27 131845.19 131849.48 131860.25\n",
      "  131871.62 131856.56 131823.66 131805.2  131794.75 131774.16 131766.77\n",
      "  131795.98 131838.11 131866.7  131886.69 131909.14 131932.2  131951.58]\n",
      " [132045.66 132049.66 132051.5  132048.73 132042.58 132031.2  132015.53\n",
      "  132002.61 131991.23 131984.47 131992.16 132001.69 132004.16 132011.53\n",
      "  132025.67 132020.75 131997.69 131981.7  131974.62 131952.5  131922.67\n",
      "  131918.05 131946.03 131983.23 132016.14 132046.28 132074.56 132097.62]\n",
      " [132200.02 132204.02 132205.25 132200.02 132192.64 132182.19 132169.58\n",
      "  132157.58 132146.2  132142.2  132148.67 132156.05 132162.2  132170.5\n",
      "  132176.03 132175.11 132172.03 132168.05 132158.2  132138.52 132109.62\n",
      "  132083.17 132081.02 132107.16 132145.6  132182.48 132214.47 132241.53]\n",
      " [132365.75 132366.06 132364.22 132358.69 132349.77 132338.08 132326.1\n",
      "  132313.48 132302.11 132299.64 132305.48 132311.02 132319.02 132331.\n",
      "  132334.08 132331.   132339.61 132350.69 132343.61 132327.02 132309.17\n",
      "  132280.27 132251.98 132252.6  132284.27 132323.62 132357.14 132386.66]\n",
      " [132542.56 132538.25 132532.72 132524.72 132513.66 132500.73 132485.98\n",
      "  132472.14 132461.69 132458.61 132464.77 132470.61 132479.22 132493.67\n",
      "  132499.5  132495.2  132503.2  132525.03 132532.72 132523.48 132512.11\n",
      "  132488.75 132449.08 132426.02 132440.16 132473.06 132504.73 132533.03]\n",
      " [132727.05 132719.05 132709.52 132696.61 132682.77 132667.7  132648.64\n",
      "  132633.58 132624.66 132619.73 132623.73 132636.03 132650.17 132662.47\n",
      "  132668.02 132665.55 132675.08 132703.67 132722.73 132719.05 132709.52\n",
      "  132693.22 132656.02 132621.58 132618.2  132637.56 132660.02 132682.77]\n",
      " [132911.23 132902.   132890.62 132873.72 132856.19 132837.12 132814.98\n",
      "  132798.08 132789.77 132784.23 132789.77 132811.61 132835.6  132847.58\n",
      "  132846.66 132844.2  132855.58 132880.48 132898.   132900.78 132900.16\n",
      "  132892.17 132861.72 132824.52 132810.06 132815.6  132824.52 132837.73]\n",
      " [133088.03 133078.5  133066.52 133048.98 133029.61 133008.7  132986.56\n",
      "  132969.66 132959.5  132959.2  132973.66 133001.02 133028.08 133040.69\n",
      "  133034.53 133028.69 133036.98 133052.67 133063.12 133073.28 133085.27\n",
      "  133083.73 133056.98 133023.77 133005.02 132997.62 132993.64 132998.25]\n",
      " [133261.14 133249.77 133234.7  133217.17 133199.03 133178.73 133160.6\n",
      "  133148.61 133143.08 133150.77 133174.75 133201.5  133222.72 133230.1\n",
      "  133219.02 133211.64 133220.56 133227.02 133229.17 133242.08 133261.14\n",
      "  133264.22 133244.23 133216.25 133195.03 133178.12 133163.06 133162.75]\n",
      " [133435.19 133418.58 133399.52 133380.75 133364.77 133349.7  133338.62\n",
      "  133336.17 133341.1  133355.23 133376.77 133397.98 133414.28 133412.73\n",
      "  133396.75 133393.67 133405.05 133402.6  133396.12 133409.97 133433.03\n",
      "  133439.48 133424.11 133399.2  133376.77 133354.   133332.48 133328.48]\n",
      " [133611.67 133589.23 133568.02 133549.27 133536.03 133530.5  133529.27\n",
      "  133533.58 133546.19 133560.02 133570.47 133582.78 133593.23 133585.23\n",
      "  133569.55 133576.02 133586.77 133573.55 133562.48 133579.08 133602.77\n",
      "  133609.53 133597.23 133574.17 133551.11 133526.2  133500.98 133496.67]\n",
      " [133790.02 133764.5  133744.52 133730.67 133722.98 133725.75 133730.98\n",
      "  133736.52 133747.6  133757.12 133757.12 133758.66 133760.5  133750.66\n",
      "  133744.52 133757.73 133763.27 133745.12 133734.98 133749.12 133764.5\n",
      "  133769.72 133762.03 133742.05 133716.53 133689.17 133668.25 133669.17]\n",
      " [133970.52 133948.06 133932.69 133926.55 133925.62 133931.77 133938.53\n",
      "  133941.   133944.69 133946.22 133937.61 133930.23 133928.69 133925.\n",
      "  133926.55 133935.77 133933.   133916.1  133908.7  133913.62 133919.47\n",
      "  133924.08 133921.   133903.17 133875.19 133849.67 133837.98 133847.52]\n",
      " [134158.69 134143.02 134134.1  134133.17 134135.62 134139.62 134142.7\n",
      "  134139.62 134135.62 134130.1  134117.19 134106.11 134105.19 134107.03\n",
      "  134108.58 134109.19 134101.19 134086.12 134075.98 134073.22 134074.75\n",
      "  134079.67 134078.12 134061.22 134035.08 134016.64 134015.72 134030.47]\n",
      " [134357.64 134348.72 134343.48 134342.27 134341.03 134340.72 134338.27\n",
      "  134331.19 134322.58 134312.75 134299.52 134288.14 134284.77 134284.77\n",
      "  134284.77 134282.61 134272.77 134255.55 134240.48 134232.48 134231.56\n",
      "  134235.25 134233.72 134220.5  134202.05 134192.52 134198.97 134215.58]\n",
      " [134562.73 134555.05 134549.2  134542.12 134535.05 134530.14 134524.6\n",
      "  134517.53 134508.   134495.7  134481.25 134469.25 134462.48 134461.27\n",
      "  134461.27 134456.03 134442.19 134420.98 134404.06 134395.16 134392.08\n",
      "  134393.   134392.08 134383.77 134376.08 134377.   134384.69 134394.22]\n",
      " [134763.2  134753.06 134741.69 134729.08 134719.55 134713.1  134707.25\n",
      "  134701.7  134693.1  134678.03 134660.2  134645.14 134637.75 134637.75\n",
      "  134636.53 134627.   134607.62 134584.25 134567.03 134559.03 134556.58\n",
      "  134556.27 134555.05 134553.2  134555.05 134561.19 134564.58 134564.27]\n",
      " [134951.08 134937.25 134922.17 134908.03 134899.11 134893.58 134888.66\n",
      "  134886.2  134877.6  134858.22 134836.7  134818.56 134809.02 134809.02\n",
      "  134807.17 134793.03 134769.97 134745.98 134730.   134724.16 134724.16\n",
      "  134726.   134728.16 134730.   134734.61 134738.61 134735.53 134725.69]\n",
      " [135127.27 135110.97 135096.22 135084.22 135076.53 135072.23 135070.08\n",
      "  135070.08 135058.7  135034.1  135011.05 134993.52 134979.67 134974.14\n",
      "  134969.22 134953.55 134929.25 134907.11 134895.12 134894.2  134896.05\n",
      "  134900.03 134904.66 134906.5  134909.27 134908.64 134896.66 134876.98]\n",
      " [135295.77 135280.1  135266.56 135257.03 135252.11 135249.03 135248.72\n",
      "  135250.27 135237.66 135209.98 135185.69 135165.1  135144.19 135130.66\n",
      "  135122.66 135107.6  135087.61 135072.23 135066.08 135067.62 135069.77\n",
      "  135073.77 135077.77 135076.53 135075.   135071.61 135051.62 135023.03]\n",
      " [135459.05 135445.2  135434.14 135426.77 135423.69 135422.77 135423.98\n",
      "  135425.53 135415.08 135388.62 135358.19 135328.06 135300.08 135282.55\n",
      "  135271.48 135260.11 135249.03 135242.58 135240.11 135241.03 135242.27\n",
      "  135244.11 135245.66 135239.19 135230.58 135225.97 135206.6  135174.62]\n",
      " [135618.62 135606.02 135597.72 135592.19 135591.27 135594.03 135597.11\n",
      "  135598.03 135588.5  135562.67 135525.77 135486.11 135452.6  135431.67\n",
      "  135420.61 135415.69 135415.08 135415.69 135414.77 135413.53 135411.08\n",
      "  135408.   135405.55 135394.17 135380.64 135376.03 135361.58 135330.52]\n",
      " [135776.06 135765.61 135758.53 135755.77 135757.61 135763.14 135768.06\n",
      "  135768.06 135757.   135729.62 135689.05 135644.77 135606.02 135581.73\n",
      "  135572.5  135574.66 135582.03 135587.27 135585.73 135581.11 135574.66\n",
      "  135566.05 135558.98 135546.67 135532.53 135525.77 135510.1  135480.27]\n",
      " [135932.58 135924.58 135920.27 135919.66 135923.66 135929.5  135935.03\n",
      "  135935.03 135922.11 135893.22 135850.47 135802.5  135760.08 135734.25\n",
      "  135728.7  135737.02 135748.69 135754.53 135751.16 135741.62 135730.55\n",
      "  135719.17 135709.64 135699.19 135685.66 135675.2  135657.69 135632.16]]\n"
     ]
    }
   ],
   "source": [
    "print(ds['geopotential'][23][2].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4935, 0.1810, 0.0266,  ..., 0.9930, 0.0333, 0.8678])\n",
      "tensor([-0.0131, -0.6380, -0.9467,  ...,  0.9856, -0.9335,  0.7351])\n"
     ]
    }
   ],
   "source": [
    "print(original[0][1][-1])\n",
    "print(original[1][1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from models.VariableAnalyzer.datasets.dataset import CustomDataset\n",
    "dataset = CustomDataset(original[0], original[1], 4 * 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1.8671e-01, 1.7526e-01, 1.6456e-01,  ..., 3.7757e-01, 1.8123e-01,\n",
      "         2.7494e-02],\n",
      "        [1.6623e-01, 1.6128e-01, 1.5663e-01,  ..., 1.4949e-01, 4.8082e-02,\n",
      "         0.0000e+00],\n",
      "        [1.3278e-01, 1.2662e-01, 1.2060e-01,  ..., 9.5946e-02, 2.5475e-02,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [9.9636e-01, 9.9600e-01, 9.9784e-01,  ..., 9.5954e-01, 2.2724e-01,\n",
      "         7.4940e-01],\n",
      "        [9.9821e-01, 9.9800e-01, 9.7642e-01,  ..., 9.5332e-01, 4.9272e-02,\n",
      "         8.2218e-01],\n",
      "        [9.7311e-01, 9.8831e-01, 7.5980e-01,  ..., 9.4597e-01, 8.4211e-04,\n",
      "         9.1442e-01]]), tensor([[2.2350e-01, 2.1749e-01, 2.1065e-01,  ..., 3.6762e-01, 1.7511e-01,\n",
      "         1.9818e-02],\n",
      "        [1.1157e-01, 1.0535e-01, 9.9633e-02,  ..., 1.5458e-01, 4.8664e-02,\n",
      "         0.0000e+00],\n",
      "        [1.1850e-01, 1.1255e-01, 1.0675e-01,  ..., 1.0239e-01, 2.7377e-02,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [7.8148e-01, 7.4497e-01, 7.3062e-01,  ..., 9.2654e-01, 9.6733e-01,\n",
      "         5.1592e-02],\n",
      "        [4.4177e-01, 4.3155e-01, 4.2144e-01,  ..., 8.7300e-01, 9.6726e-01,\n",
      "         5.7152e-04],\n",
      "        [5.6896e-01, 5.4921e-01, 5.3433e-01,  ..., 8.6438e-01, 7.7051e-01,\n",
      "         6.4963e-01]]), tensor([[ 1.9931e+05,  1.9930e+05,  1.9929e+05,  ...,  1.9479e+05,\n",
      "          1.9262e+05,  1.9086e+05],\n",
      "        [ 1.5687e+05,  1.5685e+05,  1.5683e+05,  ...,  1.5036e+05,\n",
      "          1.4873e+05,  1.4798e+05],\n",
      "        [ 1.3142e+05,  1.3139e+05,  1.3137e+05,  ...,  1.2506e+05,\n",
      "          1.2379e+05,  1.2333e+05],\n",
      "        ...,\n",
      "        [ 5.6956e-01,  4.9769e-01,  4.6944e-01,  ...,  8.5306e-01,\n",
      "          9.3464e-01, -8.9680e-01],\n",
      "        [-1.1766e-01, -1.3808e-01, -1.5826e-01,  ...,  7.4600e-01,\n",
      "          9.3452e-01, -9.9885e-01],\n",
      "        [ 1.3786e-01,  9.8364e-02,  6.8612e-02,  ...,  7.2871e-01,\n",
      "          5.4098e-01,  2.9921e-01]]))\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "dataloader = DataLoader(dataset, 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from models.VariableAnalyzer.models.model import VariableAnalyzer\n",
    "\n",
    "model = VariableAnalyzer(100, 4 * 7, 1450, 10, 3, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2800])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print(model.get_time_seq(batch).)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(slice(0, 3, None), 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m             time_seq\u001b[38;5;241m.\u001b[39mextend(seq)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([time_seq \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))])\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_time_seq(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m))\n",
      "File \u001b[0;32m/workspace/Haea/models/VariableAnalyzer/datasets/dataset.py:44\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(item)\n\u001b[0;32m---> 44\u001b[0m     src, tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_inc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data(src), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data(tgt)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "def get_time_seq(src: torch.Tensor):\n",
    "        time_seq = []\n",
    "        for i in range(4 * 7):\n",
    "            seq = [i for _ in range(100)]\n",
    "            time_seq.extend(seq)\n",
    "        return torch.tensor([time_seq for _ in range(src.size(0))])\n",
    "\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print()\n",
    "    if i == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
